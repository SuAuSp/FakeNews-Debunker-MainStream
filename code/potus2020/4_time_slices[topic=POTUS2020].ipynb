{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time slice analysis\n",
    "\n",
    "python=3.7\n",
    "|topic      |platform   |language   |\n",
    "|-----------|-----------|-----------|\n",
    "|POTUS2020  |Twitter    |en         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 输出DataFrame时显示所有的列\n",
    "pd.set_option('display.max_columns', None)\n",
    "# 输出DataFrame时每行显示完整的内容\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load debunking community (debunking dataset)\n",
    "\n",
    "这一步只是为了取出debunking数据集，如果保存有debunking数据集文件就不必像下面一样繁琐。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归类\n",
    "ms_list = [1, 3, 4, 5, 6]    # 主流\n",
    "fn_list = [0]                   # 虚假信息\n",
    "db_list = [2, 7]                   # 辟谣\n",
    "\n",
    "def communitiy_classifying(communities, label_list, commnity_number_lists):\n",
    "    '''将社区分类。'''\n",
    "    assert len(label_list) == len(commnity_number_lists)\n",
    "    com_3type = {}\n",
    "    for i in range(len(label_list)):\n",
    "        com_3type[label_list[i]] = pd.concat([communities[no] for no in commnity_number_lists[i]], axis=0)\n",
    "    # 按入度降序排序\n",
    "    for t in com_3type.values():\n",
    "        t.sort_values(['indegree'], ascending=False, inplace=True)\n",
    "        t.reset_index(drop=True, inplace=True)\n",
    "        t.drop(['index'], axis=1, inplace=True)\n",
    "    return com_3type\n",
    "\n",
    "communities_topn = load(\"pkl/communities_topn[info=1core][topic=POTUS2020].pkl\")\n",
    "com_3type = communitiy_classifying(communities_topn, ['mainstream','fake_news','debunking'], [ms_list, fn_list, db_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/3media_retweets[topic=POTUS2020].csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 257032 entries, 2 to 1140890\n",
      "Data columns (total 83 columns):\n",
      " #   Column                                 Non-Null Count   Dtype  \n",
      "---  ------                                 --------------   -----  \n",
      " 0   id                                     257032 non-null  int64  \n",
      " 1   conversation_id                        257032 non-null  int64  \n",
      " 2   referenced_tweets.replied_to.id        0 non-null       float64\n",
      " 3   referenced_tweets.retweeted.id         257032 non-null  float64\n",
      " 4   referenced_tweets.quoted.id            0 non-null       float64\n",
      " 5   author_id                              257032 non-null  int64  \n",
      " 6   in_reply_to_user_id                    0 non-null       float64\n",
      " 7   in_reply_to_username                   0 non-null       float64\n",
      " 8   retweeted_user_id                      257032 non-null  float64\n",
      " 9   retweeted_username                     257032 non-null  object \n",
      " 10  quoted_user_id                         0 non-null       float64\n",
      " 11  quoted_username                        0 non-null       float64\n",
      " 12  created_at                             257032 non-null  object \n",
      " 13  text                                   257032 non-null  object \n",
      " 14  lang                                   257032 non-null  object \n",
      " 15  source                                 0 non-null       float64\n",
      " 16  public_metrics.impression_count        257032 non-null  int64  \n",
      " 17  public_metrics.reply_count             257032 non-null  int64  \n",
      " 18  public_metrics.retweet_count           257032 non-null  int64  \n",
      " 19  public_metrics.quote_count             257032 non-null  int64  \n",
      " 20  public_metrics.like_count              257032 non-null  int64  \n",
      " 21  reply_settings                         257032 non-null  object \n",
      " 22  edit_history_tweet_ids                 257029 non-null  object \n",
      " 23  edit_controls.edits_remaining          257014 non-null  float64\n",
      " 24  edit_controls.editable_until           257014 non-null  object \n",
      " 25  edit_controls.is_edit_eligible         257014 non-null  object \n",
      " 26  possibly_sensitive                     257032 non-null  bool   \n",
      " 27  withheld.scope                         0 non-null       float64\n",
      " 28  withheld.copyright                     11 non-null      object \n",
      " 29  withheld.country_codes                 11 non-null      object \n",
      " 30  entities.annotations                   254133 non-null  object \n",
      " 31  entities.cashtags                      10 non-null      object \n",
      " 32  entities.hashtags                      21577 non-null   object \n",
      " 33  entities.mentions                      53666 non-null   object \n",
      " 34  entities.urls                          257032 non-null  object \n",
      " 35  context_annotations                    256532 non-null  object \n",
      " 36  attachments.media                      39384 non-null   object \n",
      " 37  attachments.media_keys                 39384 non-null   object \n",
      " 38  attachments.poll.duration_minutes      0 non-null       float64\n",
      " 39  attachments.poll.end_datetime          0 non-null       object \n",
      " 40  attachments.poll.id                    0 non-null       float64\n",
      " 41  attachments.poll.options               0 non-null       object \n",
      " 42  attachments.poll.voting_status         0 non-null       object \n",
      " 43  attachments.poll_ids                   0 non-null       object \n",
      " 44  author.id                              257032 non-null  int64  \n",
      " 45  author.created_at                      257032 non-null  object \n",
      " 46  author.username                        257032 non-null  object \n",
      " 47  author.name                            257011 non-null  object \n",
      " 48  author.description                     205143 non-null  object \n",
      " 49  author.entities.description.cashtags   163 non-null     object \n",
      " 50  author.entities.description.hashtags   50457 non-null   object \n",
      " 51  author.entities.description.mentions   19623 non-null   object \n",
      " 52  author.entities.description.urls       10925 non-null   object \n",
      " 53  author.entities.url.urls               44693 non-null   object \n",
      " 54  author.url                             44693 non-null   object \n",
      " 55  author.location                        171105 non-null  object \n",
      " 56  author.pinned_tweet_id                 96822 non-null   float64\n",
      " 57  author.profile_image_url               257021 non-null  object \n",
      " 58  author.protected                       257032 non-null  bool   \n",
      " 59  author.public_metrics.followers_count  257032 non-null  int64  \n",
      " 60  author.public_metrics.following_count  257032 non-null  int64  \n",
      " 61  author.public_metrics.listed_count     257032 non-null  float64\n",
      " 62  author.public_metrics.tweet_count      257032 non-null  int64  \n",
      " 63  author.verified                        257032 non-null  bool   \n",
      " 64  author.verified_type                   0 non-null       float64\n",
      " 65  author.withheld.scope                  0 non-null       float64\n",
      " 66  author.withheld.copyright              0 non-null       float64\n",
      " 67  author.withheld.country_codes          10 non-null      object \n",
      " 68  geo.coordinates.coordinates            0 non-null       float64\n",
      " 69  geo.coordinates.type                   0 non-null       float64\n",
      " 70  geo.country                            0 non-null       float64\n",
      " 71  geo.country_code                       0 non-null       float64\n",
      " 72  geo.full_name                          0 non-null       float64\n",
      " 73  geo.geo.bbox                           0 non-null       float64\n",
      " 74  geo.geo.type                           0 non-null       float64\n",
      " 75  geo.id                                 0 non-null       float64\n",
      " 76  geo.name                               0 non-null       float64\n",
      " 77  geo.place_id                           0 non-null       float64\n",
      " 78  geo.place_type                         0 non-null       float64\n",
      " 79  matching_rules                         0 non-null       float64\n",
      " 80  __twarc.retrieved_at                   257032 non-null  object \n",
      " 81  __twarc.url                            257032 non-null  object \n",
      " 82  __twarc.version                        257032 non-null  object \n",
      "dtypes: bool(3), float64(30), int64(12), object(38)\n",
      "memory usage: 159.6+ MB\n"
     ]
    }
   ],
   "source": [
    "debunk_users = set(com_3type['debunking']['Id'])\n",
    "df_debunk = df[df['author.username'].isin(debunk_users)]\n",
    "df_debunk.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉标点符号、网址、换行等字符\n",
    "def wordopt(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\\\n', '', text) # 此处加上这句代码\n",
    "    text = re.sub('\\[.*?\\]', '', text) # 去掉中括号括起来的字符串\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text) # 去掉网址\n",
    "    text = re.sub(\"\\\\W\",\" \",text) # 去掉非单词字符\n",
    "    text = re.sub('<.*?>+', '', text) # 去掉HTML, XML标签\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # 将文本中所有标点符号删除\n",
    "    text = re.sub('\\n', '', text) # 删除换行符\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) # 删除字母数字混合的“单词”\n",
    "    return text\n",
    "\n",
    "# 去掉停用词\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "eng_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "def remove_eng_stopwords(text):\n",
    "    token_text = nltk.word_tokenize(text)\n",
    "    remove_stop = [word for word in token_text if word not in eng_stopwords]\n",
    "    join_text = ' '.join(remove_stop)\n",
    "    return join_text\n",
    "\n",
    "# 词形还原\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "def word_lemmatizer(text):\n",
    "    token_text = nltk.word_tokenize(text)\n",
    "    remove_stop = [lemm.lemmatize(w) for w in token_text]\n",
    "    join_text = ' '.join(remove_stop)\n",
    "    return join_text\n",
    "\n",
    "# 去除噪声\n",
    "from nltk.corpus import stopwords\n",
    "Word_STOPWORDS = [\"e\", \"te\", \"i\", \"me\", \"qe\", \"ne\", \"nje\", \"a\", \"per\", \"sh\", \"nga\", \"ka\", \"u\", \"eshte\", \"dhe\", \"shih\", \"nuk\",\n",
    "             \"m\", \"dicka\", \"ose\", \"si\", \"shume\", \"etj\", \"se\", \"pa\", \"sipas\", \"s\", \"t\", \"dikujt\", \"dike\", \"mire\", \"vet\",\n",
    "             \"bej\", \"ai\", \"vend\", \"prej\", \"ja\", \"duke\", \"tjeter\", \"kur\", \"ia\", \"ku\", \"ta\", \"keq\", \"dy\", \"ben\", \"bere\",\n",
    "             \"behet\", \"dickaje\", \"edhe\", \"madhe\", \"la\", \"sa\", \"gjate\", \"zakonisht\", \"pas\", \"veta\", \"mbi\", \"disa\", \"iu\",\n",
    "             \"mos\", \"c\", \"para\", \"dikush\", \"gje\", \"be\", \"pak\", \"tek\", \"fare\", \"beri\", \"po\", \"bie\", \"k\", \"do\", \"gjithe\",\n",
    "             \"vete\", \"mund\", \"kam\", \"le\", \"jo\", \"beje\", \"tij\", \"kane\", \"ishte\", \"jane\", \"vjen\", \"ate\", \"kete\", \"neper\",\n",
    "             \"cdo\", \"na\", \"marre\", \"merr\", \"mori\", \"rri\", \"deri\", \"b\", \"kishte\", \"mban\", \"perpara\", \"tyre\", \"marr\",\n",
    "             \"gjitha\", \"as\", \"vetem\", \"nen\", \"here\", \"tjera\", \"tjeret\", \"drejt\", \"qenet\", \"ndonje\", \"nese\", \"jap\",\n",
    "             \"merret\", \"rreth\", \"lloj\", \"dot\", \"saj\", \"nder\", \"ndersa\", \"cila\", \"veten\", \"ma\", \"ndaj\", \"mes\", \"ajo\",\n",
    "             \"cilen\", \"por\", \"ndermjet\", \"prapa\", \"mi\", \"tere\", \"jam\", \"ashtu\", \"kesaj\", \"tille\", \"behem\", \"cilat\",\n",
    "             \"kjo\", \"menjehere\", \"ca\", \"je\", \"aq\", \"aty\", \"prane\", \"ato\", \"pasur\", \"qene\", \"cilin\", \"teper\", \"njera\",\n",
    "             \"tej\", \"krejt\", \"kush\", \"bejne\", \"ti\", \"bene\", \"midis\", \"cili\", \"ende\", \"keto\", \"kemi\", \"sic\", \"kryer\",\n",
    "             \"cilit\", \"atij\", \"gjithnje\", \"andej\", \"siper\", \"sikur\", \"ketej\", \"ciles\", \"ky\", \"papritur\", \"ua\",\n",
    "             \"kryesisht\", \"gjithcka\", \"pasi\", \"kryhet\", \"mjaft\", \"ketij\", \"perbashket\", \"ata\", \"atje\", \"vazhdimisht\",\n",
    "             \"kurre\", \"tone\", \"keshtu\", \"une\", \"sapo\", \"rralle\", \"vetes\", \"ishin\", \"afert\", \"tjetren\", \"ketu\", \"cfare\",\n",
    "             \"to\", \"anes\", \"jemi\", \"asaj\", \"secila\", \"kundrejt\", \"ketyre\", \"pse\", \"tilla\", \"mua\", \"nepermjet\", \"cilet\",\n",
    "             \"ndryshe\", \"kishin\", \"ju\", \"tani\", \"atyre\", \"dic\", \"yne\", \"kudo\", \"sone\", \"sepse\", \"cilave\", \"kem\", \"ty\",\n",
    "             \"t'i\", \"nbsp\", \"tha\", \"re\", \"the\", \"jr\", \"t\", \"n\"]\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "text_unknows= Word_STOPWORDS\n",
    "stop.update(text_unknows)\n",
    "\n",
    "# 去除噪声字符或字符串\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    '''Removing the square brackets'''\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    '''Removing URL's'''\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''Removing the stopwords from text'''\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    '''Removing the noisy text'''\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "# 去除标点符号\n",
    "def punctuation_removal(text):\n",
    "    all_list = [char for char in text if char not in string.punctuation]\n",
    "    clean_str = ''.join(all_list)\n",
    "    return clean_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    text = wordopt(text)\n",
    "    text = remove_eng_stopwords(text)\n",
    "    text = word_lemmatizer(text)\n",
    "    text = denoise_text(text)\n",
    "    text = punctuation_removal(text)\n",
    "    return text\n",
    "\n",
    "df_txt = df_debunk\n",
    "df_txt['text'] = df_txt['text'].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'created_at' column to datetime\n",
    "df_txt['created_at'] = pd.to_datetime(df_txt['created_at'])\n",
    "# Extract date from 'created_at' column\n",
    "df_txt['date'] = df_txt['created_at'].dt.date\n",
    "# Group by date\n",
    "grouped_df = df_txt.groupby('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In each time slice, aggregate texts for each user\n",
    "time_slices = dict()\n",
    "for name, group in grouped_df:\n",
    "    time_slices[name] = group.groupby(by='author.username').agg(text=(\"text\", lambda x: ' '.join(set(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time_slices from dict into DataFrame, then save it as csv\n",
    "for k, v in time_slices.items():\n",
    "    v['date'] = k\n",
    "\n",
    "df_merge_slices = pd.concat(time_slices.values())\n",
    "df_merge_slices.sort_values(by='date', inplace=True)\n",
    "df_merge_slices.to_csv(\"data/time_slices[topic=POTUS2020][lang=en][media=debunk].csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiments detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取LIWC字典\n",
    "import liwc\n",
    "liwcPath = r'data/LIWC2015_English.dic'\n",
    "parse, category_names = liwc.load_token_parser(liwcPath)\n",
    "\n",
    "# 用LIWC对每个用户进行分析\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def liwc_analyse_ver2(text, categories=['positive','negative','affect']):\n",
    "    corpus = []\n",
    "    words = []\n",
    "\n",
    "    review = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    review = review.split()\n",
    "    review = list(category for token in review for category in parse(token))\n",
    "    statements = ' '.join(review)\n",
    "    corpus.append(statements)\n",
    "    words.append(review)\n",
    "    \n",
    "    # TF-IDF\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        X_fit = vectorizer.fit(corpus)\n",
    "        X_transformed = X_fit.transform(corpus)\n",
    "\n",
    "        features = vectorizer.get_feature_names()\n",
    "        df = pd.DataFrame(X_transformed.toarray(),columns=features)\n",
    "        result = {col: df.get(col) for col in categories}\n",
    "        result_df = pd.DataFrame(result)\n",
    "    except:\n",
    "        result_df = pd.DataFrame({k:[None] for k in categories})\n",
    "\n",
    "    return result_df.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc89a52091164bb4b590ecfada7b0a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=21514), Label(value='0 / 21514')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 并行分析\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=10)\n",
    "selected_categories = ['positive','negative','affect']\n",
    "df_merge_slices.loc[:, selected_categories] = df_merge_slices['text'].parallel_apply(liwc_analyse_ver2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxicity detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 215134 entries, 0 to 215133\n",
      "Data columns (total 8 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   author.username          215134 non-null  object \n",
      " 1   text                     215134 non-null  object \n",
      " 2   positive                 70676 non-null   float64\n",
      " 3   negative                 94250 non-null   float64\n",
      " 4   affect                   127407 non-null  float64\n",
      " 5   date                     215134 non-null  object \n",
      " 6   perspective_api_results  215121 non-null  object \n",
      " 7   toxicity                 215121 non-null  float64\n",
      "dtypes: float64(4), object(4)\n",
      "memory usage: 13.1+ MB\n"
     ]
    }
   ],
   "source": [
    "perspective_res = pd.read_csv(\"data/toxicity_of_time_slices[topic=POTUS2020][lang=en][media=debunk].csv\")\n",
    "perspective_res.info()\n",
    "\n",
    "def get_score_from_json(x):\n",
    "    # x为待处理的json字符串\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = re.search(\"'score': {'value': (.+?),\", x)\n",
    "    return float(s.group(1))\n",
    "\n",
    "perspective_res['toxicity'] = perspective_res['perspective_api_results'].apply(get_score_from_json)\n",
    "perspective_res.to_csv(\"data/toxicity_of_time_slices[topic=POTUS2020][lang=en][media=debunk].csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize daily datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(df):\n",
    "    # 将日期列转换为日期类型\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    scores = ['positive', 'negative', 'affect', 'toxicity']\n",
    "\n",
    "    # 计算每天的用户数\n",
    "    daily_user_count = df.groupby('date')['author.username'].nunique()\n",
    "\n",
    "    # 计算每种分数有多少人是None\n",
    "    score_none_count = df.groupby('date')[scores].apply(lambda x: x.isnull().sum())\n",
    "\n",
    "    # 计算每天四种分数的平均值、中位数\n",
    "    daily_mean = df.groupby('date')[scores].mean()\n",
    "    daily_median = df.groupby('date')[scores].median()\n",
    "\n",
    "    # 计算去掉极端值的平均值\n",
    "    q1 = df[scores].quantile(0.25)\n",
    "    q3 = df[scores].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    df_no_extreme = df[~((df[scores] < (q1 - 1.5 * iqr)) | (df[scores] > (q3 + 1.5 * iqr))).any(axis=1)]\n",
    "    daily_mean_no_extreme = df_no_extreme.groupby('date')[scores].mean()\n",
    "\n",
    "    # 创建一个新的DataFrame来存储这些数据\n",
    "    daily_data = pd.DataFrame({\n",
    "        'date': daily_user_count.index,\n",
    "        'user_count': daily_user_count.values,\n",
    "        'positive_none_count': score_none_count['positive'],\n",
    "        'negative_none_count': score_none_count['negative'],\n",
    "        'affect_none_count': score_none_count['affect'],\n",
    "        'toxicity_none_count': score_none_count['toxicity'],\n",
    "        'positive_mean': daily_mean['positive'].values,\n",
    "        'negative_mean': daily_mean['negative'].values,\n",
    "        'affect_mean': daily_mean['affect'].values,\n",
    "        'toxicity_mean': daily_mean['toxicity'].values,\n",
    "        'positive_median': daily_median['positive'].values,\n",
    "        'negative_median': daily_median['negative'].values,\n",
    "        'affect_median': daily_median['affect'].values,\n",
    "        'toxicity_median': daily_median['toxicity'].values,\n",
    "        'positive_mean_no_extreme': daily_mean_no_extreme['positive'].values,\n",
    "        'negative_mean_no_extreme': daily_mean_no_extreme['negative'].values,\n",
    "        'affect_mean_no_extreme': daily_mean_no_extreme['affect'].values,\n",
    "        'toxicity_mean_no_extreme': daily_mean_no_extreme['toxicity'].values,\n",
    "    })\n",
    "\n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user_count</th>\n",
       "      <th>positive_none_count</th>\n",
       "      <th>negative_none_count</th>\n",
       "      <th>affect_none_count</th>\n",
       "      <th>toxicity_none_count</th>\n",
       "      <th>positive_mean</th>\n",
       "      <th>negative_mean</th>\n",
       "      <th>affect_mean</th>\n",
       "      <th>toxicity_mean</th>\n",
       "      <th>positive_median</th>\n",
       "      <th>negative_median</th>\n",
       "      <th>affect_median</th>\n",
       "      <th>toxicity_median</th>\n",
       "      <th>positive_mean_no_extreme</th>\n",
       "      <th>negative_mean_no_extreme</th>\n",
       "      <th>affect_mean_no_extreme</th>\n",
       "      <th>toxicity_mean_no_extreme</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-08-18</th>\n",
       "      <td>2020-08-18</td>\n",
       "      <td>662</td>\n",
       "      <td>126</td>\n",
       "      <td>468</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0.090433</td>\n",
       "      <td>0.121872</td>\n",
       "      <td>0.252722</td>\n",
       "      <td>0.193076</td>\n",
       "      <td>0.092450</td>\n",
       "      <td>0.131306</td>\n",
       "      <td>0.184900</td>\n",
       "      <td>0.233432</td>\n",
       "      <td>0.089905</td>\n",
       "      <td>0.112149</td>\n",
       "      <td>0.241073</td>\n",
       "      <td>0.181239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-19</th>\n",
       "      <td>2020-08-19</td>\n",
       "      <td>604</td>\n",
       "      <td>497</td>\n",
       "      <td>106</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083402</td>\n",
       "      <td>0.080458</td>\n",
       "      <td>0.181254</td>\n",
       "      <td>0.130611</td>\n",
       "      <td>0.069007</td>\n",
       "      <td>0.083624</td>\n",
       "      <td>0.167248</td>\n",
       "      <td>0.112023</td>\n",
       "      <td>0.083402</td>\n",
       "      <td>0.080375</td>\n",
       "      <td>0.181140</td>\n",
       "      <td>0.129294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-20</th>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>1765</td>\n",
       "      <td>1347</td>\n",
       "      <td>1331</td>\n",
       "      <td>1013</td>\n",
       "      <td>0</td>\n",
       "      <td>0.063234</td>\n",
       "      <td>0.047840</td>\n",
       "      <td>0.125516</td>\n",
       "      <td>0.077897</td>\n",
       "      <td>0.064282</td>\n",
       "      <td>0.046176</td>\n",
       "      <td>0.128565</td>\n",
       "      <td>0.051811</td>\n",
       "      <td>0.063234</td>\n",
       "      <td>0.047840</td>\n",
       "      <td>0.125516</td>\n",
       "      <td>0.077897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-21</th>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>2042</td>\n",
       "      <td>1580</td>\n",
       "      <td>1031</td>\n",
       "      <td>754</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076570</td>\n",
       "      <td>0.089243</td>\n",
       "      <td>0.196090</td>\n",
       "      <td>0.164334</td>\n",
       "      <td>0.064282</td>\n",
       "      <td>0.094703</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.144296</td>\n",
       "      <td>0.076718</td>\n",
       "      <td>0.088919</td>\n",
       "      <td>0.195412</td>\n",
       "      <td>0.162565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-22</th>\n",
       "      <td>2020-08-22</td>\n",
       "      <td>525</td>\n",
       "      <td>359</td>\n",
       "      <td>374</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086816</td>\n",
       "      <td>0.087252</td>\n",
       "      <td>0.208990</td>\n",
       "      <td>0.102027</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.093250</td>\n",
       "      <td>0.212000</td>\n",
       "      <td>0.041915</td>\n",
       "      <td>0.086816</td>\n",
       "      <td>0.085356</td>\n",
       "      <td>0.207375</td>\n",
       "      <td>0.099193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  user_count  positive_none_count  negative_none_count  \\\n",
       "date                                                                          \n",
       "2020-08-18 2020-08-18         662                  126                  468   \n",
       "2020-08-19 2020-08-19         604                  497                  106   \n",
       "2020-08-20 2020-08-20        1765                 1347                 1331   \n",
       "2020-08-21 2020-08-21        2042                 1580                 1031   \n",
       "2020-08-22 2020-08-22         525                  359                  374   \n",
       "\n",
       "            affect_none_count  toxicity_none_count  positive_mean  \\\n",
       "date                                                                \n",
       "2020-08-18                 81                    0       0.090433   \n",
       "2020-08-19                 61                    0       0.083402   \n",
       "2020-08-20               1013                    0       0.063234   \n",
       "2020-08-21                754                    0       0.076570   \n",
       "2020-08-22                261                    0       0.086816   \n",
       "\n",
       "            negative_mean  affect_mean  toxicity_mean  positive_median  \\\n",
       "date                                                                     \n",
       "2020-08-18       0.121872     0.252722       0.193076         0.092450   \n",
       "2020-08-19       0.080458     0.181254       0.130611         0.069007   \n",
       "2020-08-20       0.047840     0.125516       0.077897         0.064282   \n",
       "2020-08-21       0.089243     0.196090       0.164334         0.064282   \n",
       "2020-08-22       0.087252     0.208990       0.102027         0.106000   \n",
       "\n",
       "            negative_median  affect_median  toxicity_median  \\\n",
       "date                                                          \n",
       "2020-08-18         0.131306       0.184900         0.233432   \n",
       "2020-08-19         0.083624       0.167248         0.112023   \n",
       "2020-08-20         0.046176       0.128565         0.051811   \n",
       "2020-08-21         0.094703       0.189405         0.144296   \n",
       "2020-08-22         0.093250       0.212000         0.041915   \n",
       "\n",
       "            positive_mean_no_extreme  negative_mean_no_extreme  \\\n",
       "date                                                             \n",
       "2020-08-18                  0.089905                  0.112149   \n",
       "2020-08-19                  0.083402                  0.080375   \n",
       "2020-08-20                  0.063234                  0.047840   \n",
       "2020-08-21                  0.076718                  0.088919   \n",
       "2020-08-22                  0.086816                  0.085356   \n",
       "\n",
       "            affect_mean_no_extreme  toxicity_mean_no_extreme  \n",
       "date                                                          \n",
       "2020-08-18                0.241073                  0.181239  \n",
       "2020-08-19                0.181140                  0.129294  \n",
       "2020-08-20                0.125516                  0.077897  \n",
       "2020-08-21                0.195412                  0.162565  \n",
       "2020-08-22                0.207375                  0.099193  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = perspective_res  # data contains toxicity scores and 3 kinds of sentiment scores\n",
    "daily_statistics = calculate_statistics(data)\n",
    "daily_statistics.to_csv(\"data/daily_statistics[topic=POTUS2020][lang=en][media=debunk].csv\", index=False)\n",
    "daily_statistics.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
