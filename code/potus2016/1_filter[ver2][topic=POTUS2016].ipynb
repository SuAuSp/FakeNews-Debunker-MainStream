{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POTUS Election 2016 Analysis ver2 (1)\n",
    "\n",
    "python=3.7\n",
    "\n",
    "2016 POTUS Election\n",
    "\n",
    "筛选与2016年美国总统大选有关的帖子和URL处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "from joblib import dump, load\n",
    "from joblib import Parallel, delayed  # 并行计算\n",
    "from tldextract import extract\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 筛选2016年10月10日-2017年1月20日有关的英文帖子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"MAGA\", \"MakeAmericaGreatAgain\", \"Trump\", \"AmericaFirst\", \"Make America Great Again\", \\\n",
    "             \"Hillary\", \"Clinton\", \"ImWithHer\", \"OHHillYes\", \\\n",
    "             \"election2016\", \"elections2016\", \"debates2016\"]\n",
    "\n",
    "# 处理csv原文件(编号从1到27)\n",
    "def filter(csv_file_index, keywords, source_path, target_path, result_file_suffix):\n",
    "    df = pd.read_csv(source_path + str(csv_file_index) + \".csv\")  # 从source_path文件夹中取出原文件\n",
    "    df_f1 = df[df['text'].str.contains('|'.join(keywords), case=False)]  # 对帖文进行关键词检索\n",
    "    df_f2 = df[df['entities.hashtags'].astype(str).str.contains('|'.join(keywords), case=False)]  # 对话题标签进行关键词检索\n",
    "    df_cat = pd.concat([df_f1,df_f2], axis=0, join='inner')  # 合并两种检索结果\n",
    "    df_cat.sort_index(axis=0, ascending=True, inplace=True)  # 恢复帖文顺序\n",
    "    df_cat.drop_duplicates(subset='id', keep='first', inplace=True)  # 去重\n",
    "    df_cat.to_csv(target_path + str(csv_file_index) + result_file_suffix + \".csv\", index=False)  # 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 并行处理从13到16号csv原文件\n",
    "begin = 1\n",
    "end = 1\n",
    "source_folder = \"/mnt/data/Project7/fakenews/csv/\"\n",
    "target_folder = \"data/csv_potus_election2016/\"\n",
    "suffix = \"_potus_election2016\"\n",
    "# Parallel(n_jobs=4)(delayed(filter)(ind, keywords, source_folder, target_folder, suffix) \\\n",
    "#                               for ind in range(begin, end+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并处理结果，并筛选出英文帖子\n",
    "results = []\n",
    "for i in range(begin, end+1):\n",
    "    df = pd.read_csv(target_folder + str(i) + suffix + \".csv\")\n",
    "    results.append(df)\n",
    "results.reverse()\n",
    "df_cat = pd.concat(results, axis=0)\n",
    "df_cat.drop_duplicates(subset='id', keep='first', inplace=True)  # 去重\n",
    "df_cat = df_cat[df_cat['lang'] == 'en']  # 英文帖子\n",
    "df_cat.reset_index(drop=True, inplace=True)  # 重置行索引\n",
    "df_cat.to_csv(\"data/full_data[topic=POTUS2016].csv\", index=False)  # 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1023030</th>\n",
       "      <td>2017-01-21T00:00:04.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023031</th>\n",
       "      <td>2017-01-21T00:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023032</th>\n",
       "      <td>2017-01-20T23:59:54.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023033</th>\n",
       "      <td>2017-01-20T23:59:53.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023034</th>\n",
       "      <td>2017-01-20T23:59:52.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084876</th>\n",
       "      <td>2016-10-10T13:46:20.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084877</th>\n",
       "      <td>2016-10-10T13:46:19.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084878</th>\n",
       "      <td>2016-10-10T13:46:18.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084879</th>\n",
       "      <td>2016-10-10T13:46:17.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084880</th>\n",
       "      <td>2016-10-10T13:46:17.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1061851 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at\n",
       "1023030  2017-01-21T00:00:04.000Z\n",
       "1023031  2017-01-21T00:00:00.000Z\n",
       "1023032  2017-01-20T23:59:54.000Z\n",
       "1023033  2017-01-20T23:59:53.000Z\n",
       "1023034  2017-01-20T23:59:52.000Z\n",
       "...                           ...\n",
       "2084876  2016-10-10T13:46:20.000Z\n",
       "2084877  2016-10-10T13:46:19.000Z\n",
       "2084878  2016-10-10T13:46:18.000Z\n",
       "2084879  2016-10-10T13:46:17.000Z\n",
       "2084880  2016-10-10T13:46:17.000Z\n",
       "\n",
       "[1061851 rows x 1 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cat.loc[1023030:, ['created_at']]  # 1023032-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_cat.loc[1023032:, :]\n",
    "data.to_csv(\"data/full_data[topic=POTUS2016].csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors: 387580\n",
      "in_reply_to_users: 15799\n",
      "retweeted_users: 24571\n",
      "quoted_users: 5376\n",
      "all_users: 394207\n"
     ]
    }
   ],
   "source": [
    "# 统计用户数\n",
    "# 帖子作者\n",
    "authors = set(data[data['author.username'].notna()]['author.username'])\n",
    "# 被回复者\n",
    "in_reply_to_users = set(data[data['in_reply_to_username'].notna()]['in_reply_to_username'])\n",
    "# 被转推者\n",
    "retweeted_users = set(data[data['retweeted_username'].notna()]['retweeted_username'])\n",
    "# 被引用者\n",
    "quoted_users = set(data[data['quoted_username'].notna()]['quoted_username'])\n",
    "# 涉及到的全部用户\n",
    "all_users = authors | in_reply_to_users | retweeted_users | quoted_users\n",
    "\n",
    "# 列出各种用户的数量\n",
    "print(\"authors:\", len(authors))\n",
    "print(\"in_reply_to_users:\", len(in_reply_to_users))\n",
    "print(\"retweeted_users:\", len(retweeted_users))\n",
    "print(\"quoted_users:\", len(quoted_users))\n",
    "print(\"all_users:\", len(all_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/full_data[topic=POTUS2016].csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160248"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取出帖子中的url\n",
    "def get_url_from_url_list(url_list: str, url_set: set):\n",
    "    if type(url_list) != str:\n",
    "        return url_list\n",
    "    fd = re.findall('\"(.+?)\"', url_list)\n",
    "    url_set.update(fd)\n",
    "\n",
    "url_set = set()\n",
    "df['entities.urls'].apply(get_url_from_url_list, args=(url_set, ))\n",
    "len(url_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 短url（长度小于等于23）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of short urls:  58106\n"
     ]
    }
   ],
   "source": [
    "short_urls = set()\n",
    "for a in url_set:\n",
    "    if len(a) <= 23:\n",
    "        short_urls.add(a)\n",
    "\n",
    "dump(short_urls, \"pkl/short_urls[topic=POTUS2016][type=set].pkl\")\n",
    "print(\"number of short urls: \", len(short_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "去国外服务器上跑转换短URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58106 entries, 0 to 58105\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   url           58106 non-null  object\n",
      " 1   reverted_url  58106 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 908.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# 转换后的短URL\n",
    "df_reverted_urls = pd.read_csv(\"data/reverted_short_urls[topic=POTUS2016].csv\")\n",
    "df_reverted_urls.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 长url（长度大于23）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of short urls:  102142\n"
     ]
    }
   ],
   "source": [
    "long_urls = set({})\n",
    "for a in url_set:\n",
    "    if len(a) > 23:\n",
    "        long_urls.add(a)\n",
    "\n",
    "dump(long_urls, \"pkl/long_urls[topic=POTUS2016][type=set].pkl\")\n",
    "print(\"number of short urls: \", len(long_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 102142 entries, 0 to 102141\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   url     102142 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 798.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# long_urls = load(\"pkl/long_urls[topic=POTUS2016][type=set].pkl\")\n",
    "df_long_urls = pd.DataFrame(long_urls, columns=['url'])\n",
    "df_long_urls.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 长短URL合并&标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_urls['reverted_url'] = df_long_urls['url']\n",
    "df_urls = pd.concat([df_reverted_urls, df_long_urls], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从URL中提取域名的函数\n",
    "from tldextract import extract\n",
    "def get_domain(url:str) -> str:\n",
    "    res = extract(url)\n",
    "    return res[1] + '.' + res[2]\n",
    "\n",
    "df_urls['domain'] = df_urls['reverted_url'].apply(get_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入三种媒体的域名列表\n",
    "# fake news media\n",
    "fn_df = pd.read_csv(\"data/FakeNewsDomain_from_iffy.news_23.09.06.csv\")\n",
    "# mainstream media\n",
    "ms_df = pd.read_csv(\"data/high_credibility_websites_CoVaxxy_ver2.csv\")\n",
    "# debunking media\n",
    "db_df = pd.read_csv(\"data/(Merge_ver2)debunking_fact-checking_sites.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给网址打标记\n",
    "fn_set = set(fn_df['Domain'])\n",
    "ms_set = set(ms_df['site'])\n",
    "db_set = set(db_df['domain'].dropna())\n",
    "db_re = '|'.join(db_set)\n",
    "\n",
    "df_urls['fake_news'] = ''\n",
    "df_urls['mainstream'] = ''\n",
    "df_urls['debunking'] = ''\n",
    "\n",
    "# 多进程地打标记\n",
    "def url_labeling(urls:pd.DataFrame, media, label:str):\n",
    "    '''\n",
    "    label = 'fake_news', 'mainstream' or 'debunking'\n",
    "    '''\n",
    "    if label != 'debunking':\n",
    "        for i in urls.index:\n",
    "            urls[label][i] = True if urls['domain'][i] in media else False\n",
    "    else:\n",
    "        for i in urls.index:\n",
    "            urls[label][i] = True if re.search(pattern=db_re, string=urls['reverted_url'][i]) else False\n",
    "            \n",
    "    return urls[label]\n",
    "\n",
    "# 多进程打标记\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "pool = Pool(processes=3)\n",
    "res_fn = pool.apply_async(url_labeling, (df_urls, fn_set, 'fake_news'))\n",
    "res_ms = pool.apply_async(url_labeling, (df_urls, ms_set, 'mainstream'))\n",
    "res_db = pool.apply_async(url_labeling, (df_urls, db_re, 'debunking'))\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并标记结果，保存文件\n",
    "df_urls_labeled = pd.concat([df_urls.loc[:, ['url','reverted_url','domain']], res_fn.get(), res_ms.get(), res_db.get()], axis=1)\n",
    "df_urls_labeled.to_csv(\"data/labeled_urls[topic=POTUS2016].csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
