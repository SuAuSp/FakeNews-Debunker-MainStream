{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time slice analysis\n",
    "\n",
    "python=3.7\n",
    "|topic      |platform   |language   |\n",
    "|-----------|-----------|-----------|\n",
    "|POTUS2016  |Twitter    |en         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 输出DataFrame时显示所有的列\n",
    "pd.set_option('display.max_columns', None)\n",
    "# 输出DataFrame时每行显示完整的内容\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load debunking community (debunking dataset)\n",
    "\n",
    "这一步只是为了取出debunking数据集，如果保存有debunking数据集文件就不必像下面一样繁琐。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归类\n",
    "ms_list = [0, 3, 4, 5, 6, 7]    # 主流\n",
    "fn_list = [1]                   # 虚假信息\n",
    "db_list = [2]                   # 辟谣\n",
    "\n",
    "def communitiy_classifying(communities, label_list, commnity_number_lists):\n",
    "    '''将社区分类。'''\n",
    "    assert len(label_list) == len(commnity_number_lists)\n",
    "    com_3type = {}\n",
    "    for i in range(len(label_list)):\n",
    "        com_3type[label_list[i]] = pd.concat([communities[no] for no in commnity_number_lists[i]], axis=0)\n",
    "    # 按入度降序排序\n",
    "    for t in com_3type.values():\n",
    "        t.sort_values(['indegree'], ascending=False, inplace=True)\n",
    "        t.reset_index(drop=True, inplace=True)\n",
    "        t.drop(['index'], axis=1, inplace=True)\n",
    "    return com_3type\n",
    "\n",
    "communities_topn = load(\"pkl/communities_topn[info=1core][time=16.10.10-16.12.19][topic=POTUS2016].pkl\")\n",
    "com_3type = communitiy_classifying(communities_topn, ['mainstream','fake_news','debunking'], [ms_list, fn_list, db_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26325, 84)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/3media_retweets[topic=POTUS2016][time=16.10.10-16.12.19][lang=en].csv\")\n",
    "debunk_users = set(com_3type['debunking']['Id'])\n",
    "df_debunk = df[df['author.username'].isin(debunk_users)]\n",
    "df_debunk.to_csv(\"data/debunking_retweets[topic=POTUS2016][debunking=media+keywords].csv\")\n",
    "df_debunk.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉标点符号、网址、换行等字符\n",
    "def wordopt(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\\\n', '', text) # 此处加上这句代码\n",
    "    text = re.sub('\\[.*?\\]', '', text) # 去掉中括号括起来的字符串\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text) # 去掉网址\n",
    "    text = re.sub(\"\\\\W\",\" \",text) # 去掉非单词字符\n",
    "    text = re.sub('<.*?>+', '', text) # 去掉HTML, XML标签\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # 将文本中所有标点符号删除\n",
    "    text = re.sub('\\n', '', text) # 删除换行符\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) # 删除字母数字混合的“单词”\n",
    "    return text\n",
    "\n",
    "# 去掉停用词\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "eng_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "def remove_eng_stopwords(text):\n",
    "    token_text = nltk.word_tokenize(text)\n",
    "    remove_stop = [word for word in token_text if word not in eng_stopwords]\n",
    "    join_text = ' '.join(remove_stop)\n",
    "    return join_text\n",
    "\n",
    "# 词形还原\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "def word_lemmatizer(text):\n",
    "    token_text = nltk.word_tokenize(text)\n",
    "    remove_stop = [lemm.lemmatize(w) for w in token_text]\n",
    "    join_text = ' '.join(remove_stop)\n",
    "    return join_text\n",
    "\n",
    "# 去除噪声\n",
    "from nltk.corpus import stopwords\n",
    "Word_STOPWORDS = [\"e\", \"te\", \"i\", \"me\", \"qe\", \"ne\", \"nje\", \"a\", \"per\", \"sh\", \"nga\", \"ka\", \"u\", \"eshte\", \"dhe\", \"shih\", \"nuk\",\n",
    "             \"m\", \"dicka\", \"ose\", \"si\", \"shume\", \"etj\", \"se\", \"pa\", \"sipas\", \"s\", \"t\", \"dikujt\", \"dike\", \"mire\", \"vet\",\n",
    "             \"bej\", \"ai\", \"vend\", \"prej\", \"ja\", \"duke\", \"tjeter\", \"kur\", \"ia\", \"ku\", \"ta\", \"keq\", \"dy\", \"ben\", \"bere\",\n",
    "             \"behet\", \"dickaje\", \"edhe\", \"madhe\", \"la\", \"sa\", \"gjate\", \"zakonisht\", \"pas\", \"veta\", \"mbi\", \"disa\", \"iu\",\n",
    "             \"mos\", \"c\", \"para\", \"dikush\", \"gje\", \"be\", \"pak\", \"tek\", \"fare\", \"beri\", \"po\", \"bie\", \"k\", \"do\", \"gjithe\",\n",
    "             \"vete\", \"mund\", \"kam\", \"le\", \"jo\", \"beje\", \"tij\", \"kane\", \"ishte\", \"jane\", \"vjen\", \"ate\", \"kete\", \"neper\",\n",
    "             \"cdo\", \"na\", \"marre\", \"merr\", \"mori\", \"rri\", \"deri\", \"b\", \"kishte\", \"mban\", \"perpara\", \"tyre\", \"marr\",\n",
    "             \"gjitha\", \"as\", \"vetem\", \"nen\", \"here\", \"tjera\", \"tjeret\", \"drejt\", \"qenet\", \"ndonje\", \"nese\", \"jap\",\n",
    "             \"merret\", \"rreth\", \"lloj\", \"dot\", \"saj\", \"nder\", \"ndersa\", \"cila\", \"veten\", \"ma\", \"ndaj\", \"mes\", \"ajo\",\n",
    "             \"cilen\", \"por\", \"ndermjet\", \"prapa\", \"mi\", \"tere\", \"jam\", \"ashtu\", \"kesaj\", \"tille\", \"behem\", \"cilat\",\n",
    "             \"kjo\", \"menjehere\", \"ca\", \"je\", \"aq\", \"aty\", \"prane\", \"ato\", \"pasur\", \"qene\", \"cilin\", \"teper\", \"njera\",\n",
    "             \"tej\", \"krejt\", \"kush\", \"bejne\", \"ti\", \"bene\", \"midis\", \"cili\", \"ende\", \"keto\", \"kemi\", \"sic\", \"kryer\",\n",
    "             \"cilit\", \"atij\", \"gjithnje\", \"andej\", \"siper\", \"sikur\", \"ketej\", \"ciles\", \"ky\", \"papritur\", \"ua\",\n",
    "             \"kryesisht\", \"gjithcka\", \"pasi\", \"kryhet\", \"mjaft\", \"ketij\", \"perbashket\", \"ata\", \"atje\", \"vazhdimisht\",\n",
    "             \"kurre\", \"tone\", \"keshtu\", \"une\", \"sapo\", \"rralle\", \"vetes\", \"ishin\", \"afert\", \"tjetren\", \"ketu\", \"cfare\",\n",
    "             \"to\", \"anes\", \"jemi\", \"asaj\", \"secila\", \"kundrejt\", \"ketyre\", \"pse\", \"tilla\", \"mua\", \"nepermjet\", \"cilet\",\n",
    "             \"ndryshe\", \"kishin\", \"ju\", \"tani\", \"atyre\", \"dic\", \"yne\", \"kudo\", \"sone\", \"sepse\", \"cilave\", \"kem\", \"ty\",\n",
    "             \"t'i\", \"nbsp\", \"tha\", \"re\", \"the\", \"jr\", \"t\", \"n\"]\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "text_unknows= Word_STOPWORDS\n",
    "stop.update(text_unknows)\n",
    "\n",
    "# 去除噪声字符或字符串\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    '''Removing the square brackets'''\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    '''Removing URL's'''\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''Removing the stopwords from text'''\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    '''Removing the noisy text'''\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "# 去除标点符号\n",
    "def punctuation_removal(text):\n",
    "    all_list = [char for char in text if char not in string.punctuation]\n",
    "    clean_str = ''.join(all_list)\n",
    "    return clean_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    text = wordopt(text)\n",
    "    text = remove_eng_stopwords(text)\n",
    "    text = word_lemmatizer(text)\n",
    "    text = denoise_text(text)\n",
    "    text = punctuation_removal(text)\n",
    "    return text\n",
    "\n",
    "df_txt = df_debunk\n",
    "df_txt['text'] = df_txt['text'].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'created_at' column to datetime\n",
    "df_txt['created_at'] = pd.to_datetime(df_txt['created_at'])\n",
    "# Extract date from 'created_at' column\n",
    "df_txt['date'] = df_txt['created_at'].dt.date\n",
    "# Group by date\n",
    "grouped_df = df_txt.groupby('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In each time slice, aggregate texts for each user\n",
    "time_slices = dict()\n",
    "for name, group in grouped_df:\n",
    "    time_slices[name] = group.groupby(by='author.username').agg(text=(\"text\", lambda x: ' '.join(set(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time_slices from dict into DataFrame, then save it as csv\n",
    "for k, v in time_slices.items():\n",
    "    v['date'] = k\n",
    "\n",
    "df_merge_slices = pd.concat(time_slices.values())\n",
    "df_merge_slices.sort_values(by='date', inplace=True)\n",
    "df_merge_slices.to_csv(\"data/time_slices[topic=POTUS2016][lang=en][media=debunk].csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxicity detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13683 entries, 0 to 13682\n",
      "Data columns (total 5 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   author.username          13683 non-null  object \n",
      " 1   text                     13683 non-null  object \n",
      " 2   date                     13683 non-null  object \n",
      " 3   perspective_api_results  13683 non-null  object \n",
      " 4   toxicity                 13683 non-null  float64\n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 534.6+ KB\n"
     ]
    }
   ],
   "source": [
    "perspective_res = pd.read_csv(\"data/toxicity_of_time_slices[topic=POTUS2016][lang=en][media=debunk].csv\")\n",
    "perspective_res.info()\n",
    "\n",
    "def get_score_from_json(x):\n",
    "    # x为待处理的json字符串\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = re.search(\"'score': {'value': (.+?),\", x)\n",
    "    return float(s.group(1))\n",
    "\n",
    "perspective_res['toxicity'] = perspective_res['perspective_api_results'].apply(get_score_from_json)\n",
    "perspective_res.to_csv(\"data/toxicity_of_time_slices[topic=POTUS2016][lang=en][media=debunk].csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiments detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取LIWC字典\n",
    "import liwc\n",
    "liwcPath = r'data/LIWC2015_English.dic'\n",
    "parse, category_names = liwc.load_token_parser(liwcPath)\n",
    "\n",
    "# 用LIWC对每个用户进行分析\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def liwc_analyse_ver2(text, categories=['positive','negative','affect']):\n",
    "    corpus = []\n",
    "    words = []\n",
    "\n",
    "    review = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
    "    review = review.split()\n",
    "    review = list(category for token in review for category in parse(token))\n",
    "    statements = ' '.join(review)\n",
    "    corpus.append(statements)\n",
    "    words.append(review)\n",
    "    \n",
    "    # TF-IDF\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        X_fit = vectorizer.fit(corpus)\n",
    "        X_transformed = X_fit.transform(corpus)\n",
    "\n",
    "        features = vectorizer.get_feature_names()\n",
    "        df = pd.DataFrame(X_transformed.toarray(),columns=features)\n",
    "        result = {col: df.get(col) for col in categories}\n",
    "        result_df = pd.DataFrame(result)\n",
    "    except:\n",
    "        result_df = pd.DataFrame({k:[None] for k in categories})\n",
    "\n",
    "    return result_df.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ef39ee91bb44208863f340506646cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1369), Label(value='0 / 1369'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 并行分析\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=10)\n",
    "selected_categories = ['positive','negative','affect']\n",
    "perspective_res.loc[:, selected_categories] = perspective_res['text'].parallel_apply(liwc_analyse_ver2)\n",
    "perspective_res.to_csv(\"data/time_slices[topic=POTUS2016][lang=en][media=debunk].csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize daily datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(df:pd.DataFrame):\n",
    "    # 将日期列转换为日期类型\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    scores = ['positive', 'negative', 'affect', 'toxicity']\n",
    "    # 聚集每天的数据\n",
    "    df_groupby_date = df.groupby('date')\n",
    "\n",
    "    # 计算每天的用户数\n",
    "    daily_user_count = df_groupby_date['author.username'].nunique()\n",
    "    # 计算每种分数有多少人是None\n",
    "    score_none_count = df_groupby_date[scores].apply(lambda x: x.isnull().sum())\n",
    "    # 计算每天四种分数的平均值、中位数\n",
    "    daily_mean = df_groupby_date[scores].mean()\n",
    "    daily_median = df_groupby_date[scores].median()\n",
    "\n",
    "    # 计算去掉极端值的平均值\n",
    "    def mean_no_extreme(df:pd.DataFrame):\n",
    "        q1 = df[scores].quantile(0.25)\n",
    "        q3 = df[scores].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        df_no_extreme = df[~((df[scores] < (q1 - 1.5 * iqr)) | (df[scores] > (q3 + 1.5 * iqr)))]\n",
    "        return df_no_extreme[scores].mean()\n",
    "    \n",
    "    daily_mean_no_extreme = df_groupby_date.apply(mean_no_extreme)\n",
    "\n",
    "    # 创建一个新的DataFrame来存储这些数据\n",
    "    daily_data = pd.DataFrame({\n",
    "        'date': daily_user_count.index,\n",
    "        'user_count': daily_user_count.values,\n",
    "        'positive_none_count': score_none_count['positive'],\n",
    "        'negative_none_count': score_none_count['negative'],\n",
    "        'affect_none_count': score_none_count['affect'],\n",
    "        'toxicity_none_count': score_none_count['toxicity'],\n",
    "        'positive_mean': daily_mean['positive'].values,\n",
    "        'negative_mean': daily_mean['negative'].values,\n",
    "        'affect_mean': daily_mean['affect'].values,\n",
    "        'toxicity_mean': daily_mean['toxicity'].values,\n",
    "        'positive_median': daily_median['positive'].values,\n",
    "        'negative_median': daily_median['negative'].values,\n",
    "        'affect_median': daily_median['affect'].values,\n",
    "        'toxicity_median': daily_median['toxicity'].values,\n",
    "        'positive_mean_no_extreme': daily_mean_no_extreme['positive'].values,\n",
    "        'negative_mean_no_extreme': daily_mean_no_extreme['negative'].values,\n",
    "        'affect_mean_no_extreme': daily_mean_no_extreme['affect'].values,\n",
    "        'toxicity_mean_no_extreme': daily_mean_no_extreme['toxicity'].values,\n",
    "    })\n",
    "\n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user_count</th>\n",
       "      <th>positive_none_count</th>\n",
       "      <th>negative_none_count</th>\n",
       "      <th>affect_none_count</th>\n",
       "      <th>toxicity_none_count</th>\n",
       "      <th>positive_mean</th>\n",
       "      <th>negative_mean</th>\n",
       "      <th>affect_mean</th>\n",
       "      <th>toxicity_mean</th>\n",
       "      <th>positive_median</th>\n",
       "      <th>negative_median</th>\n",
       "      <th>affect_median</th>\n",
       "      <th>toxicity_median</th>\n",
       "      <th>positive_mean_no_extreme</th>\n",
       "      <th>negative_mean_no_extreme</th>\n",
       "      <th>affect_mean_no_extreme</th>\n",
       "      <th>toxicity_mean_no_extreme</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-10-10</th>\n",
       "      <td>2016-10-10</td>\n",
       "      <td>375</td>\n",
       "      <td>169</td>\n",
       "      <td>248</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0.092290</td>\n",
       "      <td>0.074126</td>\n",
       "      <td>0.240623</td>\n",
       "      <td>0.097578</td>\n",
       "      <td>0.091670</td>\n",
       "      <td>0.060858</td>\n",
       "      <td>0.243432</td>\n",
       "      <td>0.051069</td>\n",
       "      <td>0.092290</td>\n",
       "      <td>0.065993</td>\n",
       "      <td>0.234832</td>\n",
       "      <td>0.078819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-11</th>\n",
       "      <td>2016-10-11</td>\n",
       "      <td>369</td>\n",
       "      <td>240</td>\n",
       "      <td>244</td>\n",
       "      <td>167</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086339</td>\n",
       "      <td>0.116352</td>\n",
       "      <td>0.256076</td>\n",
       "      <td>0.191134</td>\n",
       "      <td>0.089087</td>\n",
       "      <td>0.089803</td>\n",
       "      <td>0.243207</td>\n",
       "      <td>0.169385</td>\n",
       "      <td>0.070577</td>\n",
       "      <td>0.116352</td>\n",
       "      <td>0.224072</td>\n",
       "      <td>0.180552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-12</th>\n",
       "      <td>2016-10-12</td>\n",
       "      <td>85</td>\n",
       "      <td>58</td>\n",
       "      <td>62</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0.093911</td>\n",
       "      <td>0.101019</td>\n",
       "      <td>0.269947</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.091670</td>\n",
       "      <td>0.089803</td>\n",
       "      <td>0.261744</td>\n",
       "      <td>0.095684</td>\n",
       "      <td>0.093911</td>\n",
       "      <td>0.101019</td>\n",
       "      <td>0.258072</td>\n",
       "      <td>0.126182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-13</th>\n",
       "      <td>2016-10-13</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.082110</td>\n",
       "      <td>0.155778</td>\n",
       "      <td>0.342949</td>\n",
       "      <td>0.128621</td>\n",
       "      <td>0.060858</td>\n",
       "      <td>0.174078</td>\n",
       "      <td>0.348155</td>\n",
       "      <td>0.081673</td>\n",
       "      <td>0.060858</td>\n",
       "      <td>0.155778</td>\n",
       "      <td>0.342949</td>\n",
       "      <td>0.095477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-14</th>\n",
       "      <td>2016-10-14</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.091512</td>\n",
       "      <td>0.128880</td>\n",
       "      <td>0.254591</td>\n",
       "      <td>0.242763</td>\n",
       "      <td>0.091670</td>\n",
       "      <td>0.148257</td>\n",
       "      <td>0.280056</td>\n",
       "      <td>0.305024</td>\n",
       "      <td>0.107789</td>\n",
       "      <td>0.146836</td>\n",
       "      <td>0.254591</td>\n",
       "      <td>0.242763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  user_count  positive_none_count  negative_none_count  \\\n",
       "date                                                                          \n",
       "2016-10-10 2016-10-10         375                  169                  248   \n",
       "2016-10-11 2016-10-11         369                  240                  244   \n",
       "2016-10-12 2016-10-12          85                   58                   62   \n",
       "2016-10-13 2016-10-13          20                   16                   10   \n",
       "2016-10-14 2016-10-14          21                   17                   11   \n",
       "\n",
       "            affect_none_count  toxicity_none_count  positive_mean  \\\n",
       "date                                                                \n",
       "2016-10-10                138                    0       0.092290   \n",
       "2016-10-11                167                    0       0.086339   \n",
       "2016-10-12                 49                    0       0.093911   \n",
       "2016-10-13                  9                    0       0.082110   \n",
       "2016-10-14                  8                    0       0.091512   \n",
       "\n",
       "            negative_mean  affect_mean  toxicity_mean  positive_median  \\\n",
       "date                                                                     \n",
       "2016-10-10       0.074126     0.240623       0.097578         0.091670   \n",
       "2016-10-11       0.116352     0.256076       0.191134         0.089087   \n",
       "2016-10-12       0.101019     0.269947       0.138629         0.091670   \n",
       "2016-10-13       0.155778     0.342949       0.128621         0.060858   \n",
       "2016-10-14       0.128880     0.254591       0.242763         0.091670   \n",
       "\n",
       "            negative_median  affect_median  toxicity_median  \\\n",
       "date                                                          \n",
       "2016-10-10         0.060858       0.243432         0.051069   \n",
       "2016-10-11         0.089803       0.243207         0.169385   \n",
       "2016-10-12         0.089803       0.261744         0.095684   \n",
       "2016-10-13         0.174078       0.348155         0.081673   \n",
       "2016-10-14         0.148257       0.280056         0.305024   \n",
       "\n",
       "            positive_mean_no_extreme  negative_mean_no_extreme  \\\n",
       "date                                                             \n",
       "2016-10-10                  0.092290                  0.065993   \n",
       "2016-10-11                  0.070577                  0.116352   \n",
       "2016-10-12                  0.093911                  0.101019   \n",
       "2016-10-13                  0.060858                  0.155778   \n",
       "2016-10-14                  0.107789                  0.146836   \n",
       "\n",
       "            affect_mean_no_extreme  toxicity_mean_no_extreme  \n",
       "date                                                          \n",
       "2016-10-10                0.234832                  0.078819  \n",
       "2016-10-11                0.224072                  0.180552  \n",
       "2016-10-12                0.258072                  0.126182  \n",
       "2016-10-13                0.342949                  0.095477  \n",
       "2016-10-14                0.254591                  0.242763  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/time_slices[topic=POTUS2016][lang=en][media=debunk].csv\")\n",
    "daily_statistics = calculate_statistics(data)\n",
    "daily_statistics.to_csv(\"data/daily_statistics[topic=POTUS2016][lang=en][media=debunk].csv\", index=False)\n",
    "daily_statistics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 71 entries, 2016-10-10 to 2016-12-19\n",
      "Data columns (total 18 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   date                      71 non-null     datetime64[ns]\n",
      " 1   user_count                71 non-null     int64         \n",
      " 2   positive_none_count       71 non-null     int64         \n",
      " 3   negative_none_count       71 non-null     int64         \n",
      " 4   affect_none_count         71 non-null     int64         \n",
      " 5   toxicity_none_count       71 non-null     int64         \n",
      " 6   positive_mean             64 non-null     float64       \n",
      " 7   negative_mean             67 non-null     float64       \n",
      " 8   affect_mean               68 non-null     float64       \n",
      " 9   toxicity_mean             71 non-null     float64       \n",
      " 10  positive_median           64 non-null     float64       \n",
      " 11  negative_median           67 non-null     float64       \n",
      " 12  affect_median             68 non-null     float64       \n",
      " 13  toxicity_median           71 non-null     float64       \n",
      " 14  positive_mean_no_extreme  64 non-null     float64       \n",
      " 15  negative_mean_no_extreme  67 non-null     float64       \n",
      " 16  affect_mean_no_extreme    68 non-null     float64       \n",
      " 17  toxicity_mean_no_extreme  71 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(12), int64(5)\n",
      "memory usage: 10.5 KB\n"
     ]
    }
   ],
   "source": [
    "daily_statistics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "# Calculate days from the earliest date to the latest\n",
    "delta = daily_statistics['date'][-1] - daily_statistics['date'][0]\n",
    "print(delta.days + 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
